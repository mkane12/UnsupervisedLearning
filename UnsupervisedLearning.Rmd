---
title: "Getting Handsy"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(imputeMissings) # for the impute function
library(factoextra) # for the fviz functions
library(cluster) # for the agnes/diana/fanny functions
```

# Introduction: Clustering Hand Posture Data

This report explores unsupervised learning and clustering using data collected from a motion capture system. The system was used to record 14 different users performing 5 distinct hand postures with markers attached to a left-handed glove. A set of markers on the back of the glove was used to establish a local coordinate system for the hand, and 11 additional markers were attached the the thumb and fingers of the glove. 3 markers were attached to the thumb with one above the thumbnail and the other two on the knuckles. Finally, 2 markers were attached to each finger with one above the fingernail and the other in the middle of the finger. A total of 36 features were collected from the camera system. Two other variables in the dataset are the ID of the user and the posture that the user made. 

The data were partially preprocessed. First, all markers were transformed to the local coordinate system of the record containing them. Second, each transformed marker with a norm greater than 200 millimeters was eliminated. Finally, any record that contained fewer than three markers was removed. A few issues with the data are worth noting. Based on the manner in which the data were captured, it is likely that, for a given record and user, there exists a near duplicate record originating from the same user. In addition, there are many instances of missing data in the feature set. These instances were denoted with a ? in the dataset. Finally, there is the potential for imbalanced classes, as there is no guarantee that each user and/or posture is represented with equal frequency in the dataset.
The dataset contains 78,095 rows and 38 columns. Each row corresponds to a single instant or frame as recorded by the camera system. The data are represented in the following manner:

1. Class (Integer): The hand posture of the given obervation, with 1=Fist (with thumb out),
2=Stop (hand flat), 3=Point1 (point with index finger), 4=Point2 (point with index and middle fingers), 5=Grab (fingers curled as if to grab).
2. User (Integer): The ID of the user that contributed the record.
3. X0, Y0, Z0, X1, Y1, Z1, ..., X11, Y11, Z11 (Real): The x-coordinate, y-coordinate and
z-coordinate of the twelve unlabeled marker positions.

# Missing Data Imputation

We begin by imputing the missing data values. Given the knowledge of how the data was collected, we can hypothesize that there are two ways in which the data might cluster together: by user and by posture. Perhaps the users have significantly different heights and/or hand sizes, resulting in the data generated by each user to be distinct from each other. Or perhaps the hand postures are sufficiently unique such that the markers on the glove tend to be grouped together by posture, regardless of who the user is. We will examine these hypotheses to see if either one provides a reasonable way to impute the data.

```{r, echo=FALSE}
# read_csv performs much faster than read.csv for larger datasets. 
# http://yetanothermathprogrammingconsultant.blogspot.com/2016/12/reading-csv-files-in-r-readcsv-vs.html
# Note: some of the data entries are (stupidly) entered as "?"
# Note: col_type defines the type to read data as (i for integer, d for double, etc.) Older implementations of read_csv required defining column by column, but now we can just blanket everything to "d"
# https://github.com/tidyverse/readr/issues/148
# Note: User 3 is not present in the dataframe, meaning there are a total of 14 users (id 0-2, 4-15)

data <- read_csv("data/postures.csv",na=c("","NA",".","?"),col_types = cols(.default = "d"))
```

Let's start by imputing missing data by User. There are a couple of cases to consider:
1. There are some number of missing values for a single coordinate (column) for a user.
+ In this case, we will simply impute that missing value with that coordinate (column) mean.
2. All of the values for a coordinate (column) for a user are missing.
+ In this case, we will impute those missing column values with the overall mean for all coordinates for that user.

```{r, echo=FALSE}
data_imputedByUser = data.frame()

for (user in unique(data$User)) {
  user_data <- data %>% filter(User == user)
  
  # use impute function from imputeMissings package
  # https://cran.r-project.org/web/packages/imputeMissings/imputeMissings.pdf
  # this works to impute column means for missing values, but fails if column is all NA values, which happens for a few users.
  
  user_data <- impute(user_data)
  
  # for columns that have no defined values, we will impute the values as the mean of all coordinates for that user.
  # Note: we compute the mean for columns 3 onward, because those are the coordinate values (first two columns are Class and User)
  
  user_mean <- mean(as.matrix(user_data[3:ncol(user_data)]), na.rm = TRUE)
  user_data[is.na(user_data)] <- user_mean
  
  data_imputedByUser = rbind(data_imputedByUser, user_data)
}

# and now we have a data frame with missing values imputed by user: data_imputedByUser
```

But what if the data is grouped by posture instead? Let's group the data by Class, then impute missing values by that. Again, we consider the same cases as before: 
1. There are some number of missing values for a single coordinate (column) for a posture.
+ In this case, we will simply impute that missing value with that coordinate (column) mean.
2. All of the values for a coordinate (column) for a posture are missing.
+ In this case, we will impute those missing column values with the overall mean for all coordinates for that posture.

```{r, echo=FALSE}
data_imputedByPosture <- data.frame()

for (posture in unique(data$Class)) {
  posture_data <- data %>% filter(Class == posture)
  posture_data <- impute(posture_data)
  
  # again, it is necessary to impute completely NA columns
  
  posture_mean <- mean(as.matrix(posture_data[3:ncol(posture_data)]), na.rm = TRUE)
  posture_data[is.na(posture_data)] <- posture_mean
  
  data_imputedByPosture <- rbind(data_imputedByPosture, posture_data)
}

# and now we have a data frame with missing values imputed by posture: data_imputedByPosture
```

# Clustering with k-means

Now that the missing values have been imputed, we can investigate our hypotheses by examining how well the data clusters by user and by posture. We will first cluster using k-means with 14 centroids (because there are 14 users) on the user-grouped data.

```{r, echo=FALSE}
# Rather than running the algorithm on the entire dataset (for which visualization will be unwieldy), we will run it on a random sample of 2,000 observations. 

set.seed(12)
sample_userData <- data_imputedByUser[sample(nrow(data_imputedByUser), 2000), ]
cluster_target <- sample_userData[,'User']
cluster_userFeatures <- sample_userData[, 3:ncol(data_imputedByUser)]

# standardize data using scale to center numeric columns

cluster_userFeatures <- scale(cluster_userFeatures) 

# run kmeans with 14 centroids (for the 14 users)

users_km = kmeans(cluster_userFeatures, centers = 14)

# finally, let's visualize these clusters

fviz_cluster(users_km, data = cluster_userFeatures, geom = "point", main = 'K-Means Clustering K=14')
```

It would appear that the first two components explain less than 25% of the variation in the data, and that the 14 clusters overlap quite a bit, preventing the formation of distinct clusters of users. 

We then compare these results to the actual user classifications to determine how well kmeans performed. 

```{r, echo=FALSE}
# Sort clusters by percentage breakdown from least number of users to greatest to compare directly between k-means clusters and true user clusters.

sorted_km <- sort(users_km$size)/2000.
sorted_sample <- sort(as.vector(table(sample_userData$User)))/2000.

sorted_df <- data.frame(user = c(1:length(sorted_km), 1:length(sorted_sample)), 
                        amount = c(sorted_km, sorted_sample), 
                        clusters = c(rep("k-means", length(sorted_km)), 
                                     rep("target", length(sorted_sample))))

ggplot(data = sorted_df, aes(user, amount, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  xlab("User Number") + ylab("Percentage of Users") +
  ggtitle("Sorted User Cluster Groupings According to K-Means and Target Data")
```

Looking at the bar plot, it does not appear that the k-means method captures the true clustering of data, especially for those clusters that have far fewer users than others, though it comes close for clusters with more users. 

Next, we run k-means with five centroids (for the five postures) on the data grouped by posture.

```{r, echo=FALSE}
set.seed(12)

# take random sample of 2,000 observations
# we need to do this a second time, since we're drawing from the dataframe imputed by posture rather than by user.

sample_postureData <- data_imputedByPosture[sample(nrow(data_imputedByPosture), 2000), ]
cluster_target <- sample_postureData[,'Class']
cluster_postureFeatures <- sample_postureData[, 3:ncol(data_imputedByPosture)]
cluster_postureFeatures <- scale(cluster_postureFeatures)

# run k-means with 5 centroids for the 5 postures

posture_km <- kmeans(cluster_postureFeatures, centers = 5)

# visualize result

fviz_cluster(posture_km, data = cluster_postureFeatures, geom = "point", main = 'K-Means Clustering K=5')
```

The two principal components here explain just under 30% of the variance in the data. The clusters look slightly more separated than those of the user-grouped data. However, this could also simply be a product of there being fewer clusters made from the posture-grouped data.

```{r, echo=FALSE}
# sort clusters by percentage breakdown of users in each

sorted_km <- sort(posture_km$size)/2000.
sorted_sample <- sort(as.vector(table(sample_postureData$Class)))/2000.

sorted_df <- data.frame(posture = c(1:5, 1:5), amount = c(sorted_km, sorted_sample), 
                       clusters = c(rep("kmeans", 5), rep("target", 5)))

ggplot(data = sorted_df, aes(posture, amount, fill=factor(clusters))) +
  geom_bar(stat="identity", position="dodge") +
  xlab("Class") + ylab("Percentage of Users") +
  ggtitle("Sorted Class Cluster Groupings According to Kmeans and Target Data")
```

We see that the k-means clusters seem to more closely match the target clusters when grouped by  posture (the most glaring exceptions being postures 1 and 5), and they also yield less crowded clustering. This suggests that the data clusters by class rather than by user, which makes sense, since one user's hand gestures would look more or less like those of another user. In other words, fists appear similar regardless of whose they may be, while an individual's fist looks different from their open palm.

##Evaluating Clustering

But we must also determine the most appropriate number of clusters for our model. There are several methods we can try, then compare results.

###Elbow Method

One of the simpler methods is known as the elbow method. In this method, we determine the percentage of variance explained by the k-means models with a variety of k-values (clusters). We will find that the first few clusters will explain a great deal of variance, but at some point the marginal gain will drop. By graphing the percentages of variance explained with each number of clusters, we will see a change in slope, resulting in an elbow-like shape.

```{r, echo=FALSE}
fviz_nbclust(cluster_postureFeatures, kmeans, method = "wss")
```

We notice the most significant change in slopes at `k = 2` or perhaps `k = 3`. In other words, much less variance is explained by adding a fourth cluster compared to having the first three, so the elbow method would suggest that the optimal value for `k` is `2` or `3`.

###Silhouette Method

However, the elbow method can be ambiguous and not terribly reliable, so we can also consult the silhouette method: we compare average "silhouette widths" across different numbers of clusters. The silhouette width ranges from -1 to 1 and is a measure of how similar an object is to its own cluster compared to other clusters, where a high value indicates that a given observation is well matched to its own cluster and poorly matched to neighboring clusters. If most observations have a high silhouette width, then the clustering configuration is appropriate. If many points have low or negative values, then the configutation may have too many or too few clusters.

```{r, echo=FALSE}
fviz_nbclust(cluster_postureFeatures, kmeans, method = "silhouette")
```

Here, we see that 3 clusters yields the highest average silhouette width of a bit more than 0.10. Both methods suggest that `3` clusters would be optimal for clustering using k-means.

# Other Clustering Algorithms

3 clusters might work best for k-means, but what about other clustering methods? We'll experiment with a few on the data grouped by postures and see what patterns we can observe.

##Agglomerative Hierarchical Clustering
Hierarchical clustering is a method of clustering in which similar observations are grouped into clusters. In the case of agglomerative hierarchical clustering, it begins by treating each observation as a separate cluster, then repeatedly executes the following steps:
1. Identify the two clusters that are closest together
2. Merge the two most similar clusters

Let's try this first.

```{r, echo=FALSE}
# in order to perform agglomerative clustering, we'll use the agnes function
# we'll use Ward's Method, as it's known for creating compact, even-sized clusters, and as we saw above, the true posture data is grouped fairly evenly.

features_agnes = agnes(cluster_postureFeatures, method = "ward", stand = TRUE)

pltree(features_agnes, main = "AGNES Fit of Clusters", xlab = "Position Data", sub = "")
rect.hclust(features_agnes, k = 5)
```

Looking at how the branches of the tree split, it is easy to distinguish five distinct clusters at roughly `Height = 70`.

```{r, echo=FALSE}
group_agnes = cutree(features_agnes, k = 5)
fviz_cluster(list(data = cluster_postureFeatures, cluster = group_agnes), 
             geom = "point", main = "AGNES Cluster Plot")
```

Again, the two principal components here explain just under 30% of the variance in the data. The scatterplot, however, shows quite a bit of overlap between the 5 clusters, the notable exception being cluster 1.

##Divisive Hierarchical Clustering

In addition to agglomerative hierarchical clustering, there is also divisive hierarchical clustering, which essentially takes the same steps as agglomerative clustering but in the reverse order: it begins by grouping all observations into a single cluster, and then successively splitting the clusters.

We'll try that next.

```{r, echo=FALSE}
# in order to perform divisive clustering, we'll use the diana function
# there's no option to choose mehod here, so we'll just standardize the column values before calculating dissimilarities

features_diana = diana(cluster_postureFeatures, stand = TRUE)

pltree(features_diana, main = "DIANA Fit of Clusters", xlab = "Position Data", sub = "")
rect.hclust(features_diana, k = 3)
```

Here, it looks like divisive clustering yields 3 clusters from the tree plot. But does the scatterplot give any other information?

```{r, echo=FALSE}
group_diana = cutree(features_diana, k = 3)
fviz_cluster(list(data = cluster_postureFeatures, cluster = group_diana), 
             geom = "point", main = "DIANA Cluster Plot K=3")
```

There is still quite a bit of overlap, though the two principal components still explain just under 30% of the data's variance.

Out of curiosity, let's take a look at the scatterplot for divisive clustering with 5 clusters.

```{r, echo=FALSE}
group_diana5 = cutree(features_diana, k = 5)
fviz_cluster(list(data = cluster_postureFeatures, cluster = group_diana5), 
             geom = "point", main = "DIANA Cluster Plot K=5")
```

Still a great deal of overlap and crowding in the center, and it seems that cluster 5 consists of just one observation, so five clusters does not look great for divisive clustering at all.

What if we try clustering by the 14 users instead?

```{r, echo=FALSE}
group_diana14 = cutree(features_diana, k = 14)
fviz_cluster(list(data = cluster_postureFeatures, cluster = group_diana14), 
             geom = "point", main = "DIANA Cluster Plot K=14")
```

Quite a bit of overlap, as would make sense when we add more clusters. What is more concerning are the clusters that appear to have only one observation. We know there are more than one obersvation for each user, so this seems to not reflect the actual data clustering at all.

##Fuzzy Clustering

Other than hierarchical clustering, we can also try soft clustering, or fuzzy clustering, which allows observations to be a part of more than one cluster. Given the amount of overlap we have seen thus far, this could be a useful way to cluster our observations.

```{r, echo = FALSE}
# In fuzzy clustering, each observation is "spread out" over the various clusters. Memberships are nonnegative and sum to 1 for any fixed observation.
# the membership exponent (memb.exp) ranges from 1 to Inf. When it's close to 1, we get crisper clusterings, but as it increases, we approach complete fuzzyness. 
# Note: When running on the default memb.exp = 1.3 or higher, we got an error warning us that the clusters are approaching complete fuzziness, so we should decrease memb.exp. But if we run on memb.exp = 1.2, convergence is slow, so we go for the happy medium of 1.25

features_fanny = fanny(cluster_postureFeatures, k = 5, memb.exp = 1.25)

fviz_silhouette(features_fanny)
```

Despite attempting to have 5 clusters, the `fanny` function seems to prefer 3, as we saw  with the `diana` function as well. The average silhouette width of 0.09 is a bit low, but quite a few widths look to be on the higher positive side, so 3 clusters seems perfectly appropriate.

It is also worth noting that cluster 1 is overwhelmingly prefered to the other two - perhaps adding more clusters will help spread the wealth a bit?

```{r, echo = FALSE}
# We'll try for the number of users: k = 14
# Note: we had to lower memb.exp to 1.1

features_fanny14 = fanny(cluster_postureFeatures, k = 14, memb.exp = 1.1)

fviz_silhouette(features_fanny14)
```

Cluster 1 remains more favored than the other clusters, but the average silhouette width has significantly increased to 0.12 from 0.09, so perhaps clustering by user works better for fuzzy clustering. 





